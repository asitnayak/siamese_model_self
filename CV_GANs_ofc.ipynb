{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# L11: Generative models and Gans"
      ],
      "metadata": {
        "id": "kIz8B49jeHcn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5dca29a"
      },
      "source": [
        "\n",
        "###Problem Statement:\n",
        "\n",
        "`You are working as a Data Scientist at Kyoto Animation, a Japanese animation studio`\n",
        "\n",
        "\n",
        "* In 2020, about 179 new animes were released and total more than 4505 anime have been released.  \n",
        "\n",
        "- Anime industry wants to develop an automated system to generate newer anime characters.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1mHR9Ud9bFz2PtsPRnlfGIQlp11fODR5N\" style=\"width:480px; margin-bottom:32px\"/>\n",
        "\n",
        "#### How would a human create new anime characters ?\n",
        "1. Character Profile: Choose the artistic style, skin tone, hairstyle and gesture.\n",
        "2. Rough Character Sketches\n",
        "3. Developing the Character Design\n",
        "4. Coloring an Anime Character"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QJZaM86KebJl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53n27McSMUei"
      },
      "source": [
        "## How does the data look?\n",
        "\n",
        "### Does the dataset have any labels?\n",
        "- No, since we are going to work with unsupervised machine learning, we won't be needing any labels to train our network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StomdHHrpld6"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1tkKn01cnF3MH7-8mQzIay7ShMcgdZXF7"
      ],
      "metadata": {
        "id": "XaGLMSaago8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy3MfD1ZvyOZ"
      },
      "outputs": [],
      "source": [
        "# unzip the dataset in local directory\n",
        "!unzip '/content/animefacedataset.zip' -d '/content/animefacedataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CzmEg4gMqNn"
      },
      "source": [
        "### How many samples in the data?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6ql_cACwLQL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_dimensions = (64, 64)\n",
        "batch_size = 256\n",
        "# set path to the dataset\n",
        "dataset_path = \"./animefacedataset\"\n",
        "\n",
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path, label_mode=None, image_size=image_dimensions, batch_size=batch_size\n",
        ")\n",
        "\n",
        "# scaling images to -1 to 1\n",
        "dataset = dataset.map(lambda x: (x - 127.5) / 127.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JVmn_Q0Mcq_"
      },
      "source": [
        "\n",
        "### Is the data enough?\n",
        "\n",
        "### What is the dimension of each sample image?\n",
        "- Letâ€™s visualize some samples and check the image dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_0V_oQx-aOA"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "# Display grid of images from dataset\n",
        "def display_images(total=9): # default total images to display = 9\n",
        "    num=total\n",
        "    for x in dataset:\n",
        "        pyplot.subplot(330 + 1 + total - num)\n",
        "        plt.imshow(( (x.numpy()*0.5 + 0.5) * 255).astype(\"int32\")[0]) #pyplot.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "        num-=1\n",
        "        if not num:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AktQNm91-3A7"
      },
      "outputs": [],
      "source": [
        "display_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC1nMQzotT8V"
      },
      "outputs": [],
      "source": [
        "def display_single(dataloader):\n",
        "    for x in dataloader:\n",
        "        plt.axis(\"off\")\n",
        "        print(\"Image Dimensions: \",x.numpy().shape)\n",
        "        plt.imshow(((x.numpy()*0.5 + 0.5) * 255).astype(\"int32\")[0])\n",
        "        break\n",
        "\n",
        "display_single(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import time\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten, Dropout\n",
        "from keras.layers import Input, merge\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "img_shape = (64, 64, 3)"
      ],
      "metadata": {
        "id": "VizYAV0OpyPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73weXYmYTyzK"
      },
      "source": [
        "## discriminative model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGJtShPAsIwH"
      },
      "outputs": [],
      "source": [
        "def get_disc_normal(image_shape=(64,64,3)):\n",
        "    image_shape = image_shape\n",
        "\n",
        "    dropout_prob = 0.4\n",
        "\n",
        "    #kernel_init = RandomNormal(mean=0.0, stddev=0.01)\n",
        "    kernel_init = 'glorot_uniform'\n",
        "\n",
        "    dis_input = Input(shape = image_shape)\n",
        "\n",
        "    discriminator = Conv2D(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(dis_input)\n",
        "    discriminator = LeakyReLU(0.2)(discriminator)\n",
        "    #discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator)\n",
        "\n",
        "    #discriminator = Dropout(dropout_prob)(discriminator)\n",
        "    discriminator = Conv2D(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
        "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
        "    discriminator = LeakyReLU(0.2)(discriminator)\n",
        "    #discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator)\n",
        "\n",
        "    #discriminator = Dropout(dropout_prob)(discriminator)\n",
        "    discriminator = Conv2D(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
        "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
        "    discriminator = LeakyReLU(0.2)(discriminator)\n",
        "    #discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator)\n",
        "\n",
        "    #discriminator = Dropout(dropout_prob)(discriminator)\n",
        "    discriminator = Conv2D(filters = 512, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
        "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
        "    discriminator = LeakyReLU(0.2)(discriminator)\n",
        "    #discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator)\n",
        "\n",
        "    discriminator = Flatten()(discriminator)\n",
        "\n",
        "    #discriminator = MinibatchDiscrimination(100,5)(discriminator)\n",
        "    discriminator = Dense(1)(discriminator)\n",
        "    discriminator = Activation('sigmoid')(discriminator)\n",
        "\n",
        "    discriminator_model = Model(dis_input, discriminator)\n",
        "    discriminator_model.summary()\n",
        "    return discriminator_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrhTK1cLHc1M"
      },
      "outputs": [],
      "source": [
        "discriminator = get_disc_normal()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative model"
      ],
      "metadata": {
        "id": "8NNidFnDen_z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXOGVdMzepe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2pH6Q6ysKyl"
      },
      "outputs": [],
      "source": [
        "latent_dim = 128\n",
        "\n",
        "def get_gen_normal(noise_shape = (1,1,128)):\n",
        "    noise_shape = noise_shape\n",
        "    \"\"\"\n",
        "    Changing padding = 'same' in the first layer makes a lot fo difference!!!!\n",
        "    \"\"\"\n",
        "    #kernel_init = RandomNormal(mean=0.0, stddev=0.01)\n",
        "    kernel_init = 'glorot_uniform'\n",
        "\n",
        "    gen_input = Input(shape = noise_shape) #if want to directly use with conv layer next\n",
        "    #gen_input = Input(shape = [noise_shape]) #if want to use with dense layer next\n",
        "\n",
        "    generator = Conv2DTranspose(filters = 512, kernel_size = (4,4), strides = (1,1), padding = \"valid\", data_format = \"channels_last\", kernel_initializer = kernel_init)(gen_input)\n",
        "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
        "    generator = LeakyReLU(0.2)(generator)\n",
        "\n",
        "    generator = Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
        "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
        "    generator = LeakyReLU(0.2)(generator)\n",
        "\n",
        "    generator = Conv2DTranspose(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
        "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
        "    generator = LeakyReLU(0.2)(generator)\n",
        "\n",
        "    generator = Conv2DTranspose(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
        "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
        "    generator = LeakyReLU(0.2)(generator)\n",
        "\n",
        "    generator = Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
        "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
        "    generator = LeakyReLU(0.2)(generator)\n",
        "\n",
        "    generator = Conv2DTranspose(filters = 3, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
        "    generator = Activation('tanh')(generator)\n",
        "\n",
        "    generator_model = Model(gen_input, generator)\n",
        "    generator_model.summary()\n",
        "\n",
        "    return generator_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCe83FXsHamF"
      },
      "outputs": [],
      "source": [
        "generator = get_gen_normal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhcnmYvOuEPL"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "iDOb-Lk-eusc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQbzjpfEewlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM85nIpAsvf0"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, 1, 1, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # # Assemble labels discriminating real from fake images\n",
        "        # labels = tf.concat(\n",
        "        #     [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        # )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.zeros((batch_size, 1)), tf.ones((batch_size, 1))], axis=0\n",
        "        )\n",
        "\n",
        "        # Add random noise to the labels - important trick!\n",
        "        # labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, 1, 1, self.latent_dim))\n",
        "\n",
        "        # # Assemble labels that say \"all real images\"\n",
        "        # misleading_labels = tf.zeros((batch_size, 1))\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.ones((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function save_img_batch will save the batch of images generated by GANS."
      ],
      "metadata": {
        "id": "21ApM4ikX9w3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDwHqUykJfNP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "def save_img_batch(img_batch,img_save_dir):\n",
        "    img_batch.numpy()\n",
        "    plt.figure(figsize=(4,4))\n",
        "    gs1 = gridspec.GridSpec(4, 4)\n",
        "    gs1.update(wspace=0, hspace=0)\n",
        "    rand_indices = np.random.choice(img_batch.shape[0],16,replace=False)\n",
        "    #print(rand_indices)\n",
        "    for i in range(16):\n",
        "        #plt.subplot(4, 4, i+1)\n",
        "        ax1 = plt.subplot(gs1[i])\n",
        "        ax1.set_aspect('equal')\n",
        "        rand_index = rand_indices[i]\n",
        "        image = img_batch[rand_index, :,:,:]\n",
        "        fig = plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        fig.axes.get_xaxis().set_visible(False)\n",
        "        fig.axes.get_yaxis().set_visible(False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(img_save_dir,bbox_inches='tight',pad_inches=0)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GANS are very difficult to train. So we will be monitoring the output of the GANS generator after every epoch and save the batch of generated images"
      ],
      "metadata": {
        "id": "7ra2s2-dYP-1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp0FFXGCsx-g"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=16, latent_dim=128, file_writer=None):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "        self.file_writer = file_writer\n",
        "        self.random_latent_vectors = tf.random.normal(shape=(self.num_img, 1, 1, self.latent_dim))\n",
        "        # Directory to save generated outputs after each epoch\n",
        "        self.path = './generate_per_epoch'\n",
        "        if not os.path.exists(self.path):\n",
        "            os.mkdir(self.path)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # random_latent_vectors = tf.random.normal(shape=(self.num_img, 1, 1, self.latent_dim))\n",
        "        generated_images = self.model.generator(self.random_latent_vectors)\n",
        "        generated_images = (generated_images*0.5 + 0.5)\n",
        "        # generated_images.numpy()\n",
        "        save_img_batch(generated_images, self.path + '/' + 'generated_img_%03d.png' % (epoch) )\n",
        "\n",
        "        # Convert to image and log\n",
        "        with self.file_writer.as_default():\n",
        "            tf.summary.image(\"Epoch end generated data\", generated_images, step=epoch)\n",
        "\n",
        "        # img = keras.preprocessing.image.array_to_img(generated_images)\n",
        "        # img.save(\"generated_img_%03d.png\" % (epoch))\n",
        "        # for i in range(self.num_img):\n",
        "        #     img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "        #     img.save(\"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- You need to save the logs in the tensorboard so that it can be viewed and loaded any time"
      ],
      "metadata": {
        "id": "v1AJTFlSbB6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9bGW5uH7hrA"
      },
      "outputs": [],
      "source": [
        "training_log_dir = './logs/training_logs'\n",
        "epoch_end_logdir = './logs/epoch_end_logs'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iuzAE-rae1KO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4RA79jWap-G"
      },
      "outputs": [],
      "source": [
        "epochs = 60  #  try ~100 epochs\n",
        "latent_dim = 128\n",
        "# training_log_dir = \"./drive/MyDrive/Datasets - DSML/IntroToGANs/logs/training_logs\"\n",
        "\n",
        "# Sets up a timestamped log directory.\n",
        "# epoch_end_logdir = \"./drive/MyDrive/Datasets - DSML/IntroToGANs/logs/epoch_end_logs\"\n",
        "# Creates a file writer for the log directory.\n",
        "file_writer = tf.summary.create_file_writer(epoch_end_logdir)\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.00015, beta_1=0.5),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=training_log_dir, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Start training the GANS and observe the generated output after every epoch"
      ],
      "metadata": {
        "id": "NXmOKs5-bkjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdzTRhmj3zrz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history = gan.fit(\n",
        "                    dataset, epochs=epochs,\n",
        "                    callbacks=[GANMonitor(num_img=16, latent_dim=latent_dim, file_writer=file_writer),\n",
        "                             tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfWIjqCcttU5"
      },
      "source": [
        "**How the loss changes over time?**\n",
        "- Visualizing losses is quite useful for debugging the training process.\n",
        " > For GANs, we expect: the generator's loss to reduce over time, without the discriminator's loss getting too high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avqS-OyuGPht"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6uwVQ5aller"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "training_log_dir = './logs/training_logs/train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzG6-FudE9jH"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir={training_log_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBztj4rG-poy"
      },
      "source": [
        "### Why did discriminator loss increase?\n",
        "\n",
        "- Discriminator consist of two loss parts (1st: detect real image as real; 2nd detect fake image as fake). 'Full discriminator loss' is sum of these two parts.\n",
        "\n",
        "- The loss should be as small as possible for both the generator and the discriminator. But there is a catch: the smaller the discriminator loss becomes, the more the generator loss increases and vice versa.\n",
        "\n",
        "- The images are getting more realistic (which is all we really care about) so the generator's loss is improving, while the discriminator is doing the same quality job, but getting tougher data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yTD-GpT9MK-"
      },
      "outputs": [],
      "source": [
        "epoch_end_logdir = './logs/epoch_end_logs'\n",
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir={epoch_end_logdir}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pretrained_weights = \"/content/pretrained_weights\"\n",
        "if not os.path.exists(pretrained_weights):\n",
        "  os.makedirs(pretrained_weights)\n"
      ],
      "metadata": {
        "id": "z0BKHQzHjKYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amdeSIUKEQP2"
      },
      "outputs": [],
      "source": [
        "# Load Model Weights if required\n",
        "generator.save(os.path.join(pretrained_weights,'generator.h5'))\n",
        "discriminator.save(os.path.join(pretrained_weights,'discriminator.h5'))\n",
        "\n",
        "#generator     = tf.keras.models.load_model(\"/content/drive/MyDrive/DSML_Course_Curriculum/Intro_to_GANs/pretrained_weights/generator.h5\")\n",
        "#discriminator = tf.keras.models.load_model(\"/content/drive/MyDrive/DSML_Course_Curriculum/Intro_to_GANs/pretrained_weights/discriminator.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IGHRgtDk2iX"
      },
      "source": [
        "### Full Training as a GIF:\n",
        "Here's how the generated images look, after every epoch of training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTsiU2lDlOSz"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "# loading all generated images\n",
        "images = []\n",
        "gen_img_path =  '/content/generate_per_epoch/'\n",
        "filenames = [os.path.join(gen_img_path, f) for f in os.listdir(gen_img_path) if 'generated' in f]\n",
        "filenames.sort()\n",
        "\n",
        "# Saving as gif file\n",
        "for filename in filenames:\n",
        "    images.append(imageio.imread(filename))\n",
        "imageio.mimsave('/content/movie.gif', images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZxQ5R3hKA8j"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "# Display gif\n",
        "Image(open('/content/movie.gif','rb').read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning"
      ],
      "metadata": {
        "id": "GD9qo29Qe5Hu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-39JY4CAfgT"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 512\n",
        "epochs = 60\n",
        "latent_dim = 100\n",
        "image_dimensions = (64, 64)\n",
        "\n",
        "# dataloader\n",
        "dataset_path = \"./animefacedataset\"\n",
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path, label_mode=None, image_size=image_dimensions, batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Model initialization\n",
        "generator = get_gen_normal(noise_shape = (1,1,latent_dim))\n",
        "discriminator = get_disc_normal()\n",
        "\n",
        "# path to log folder\n",
        "training_log_dir = './mode_collapse_logs/training_logs'\n",
        "epoch_end_logdir = './mode_collapse_logs/epoch_end_logs'\n",
        "\n",
        "# Sets up a timestamped log directory.\n",
        "# Creates a file writer for the log directory.\n",
        "file_writer = tf.summary.create_file_writer(epoch_end_logdir)\n",
        "\n",
        "# callbacks\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.00015, beta_1=0.5),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=training_log_dir, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmn_n--Xsz3p"
      },
      "outputs": [],
      "source": [
        "history = gan.fit(\n",
        "                  dataset, epochs=epochs,\n",
        "                  callbacks=[GANMonitor(num_img=16, latent_dim=latent_dim, file_writer=file_writer),\n",
        "                             tensorboard_callback])"
      ]
    }
  ]
}